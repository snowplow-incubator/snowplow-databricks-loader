/*
 * Copyright (c) 2012-present Snowplow Analytics Ltd.
 * All rights reserved.
 *
 * This software is made available by Snowplow Analytics, Ltd.,
 * under the terms of the Snowplow Limited Use License Agreement, Version 1.1
 * located at https://docs.snowplow.io/limited-use-license-1.1
 * BY INSTALLING, DOWNLOADING, ACCESSING, USING OR DISTRIBUTING ANY PORTION
 * OF THE SOFTWARE, YOU AGREE TO THE TERMS OF SUCH LICENSE AGREEMENT.
 */

package com.snowplowanalytics.snowplow.databricks

import cats.effect.testing.specs2.CatsEffect
import cats.effect.IO
import com.comcast.ip4s.Port
import org.apache.parquet.hadoop.metadata.CompressionCodecName
import org.specs2.Specification

import java.nio.file.Paths
import scala.concurrent.duration.DurationInt

import com.snowplowanalytics.snowplow.runtime.{AcceptedLicense, ConfigParser, HttpClient, Retrying, Telemetry, Webhook}
import com.snowplowanalytics.snowplow.sinks.kafka.KafkaSinkConfig
import com.snowplowanalytics.snowplow.sources.kafka.KafkaSourceConfig

class KafkaConfigSpec extends Specification with CatsEffect {
  import KafkaConfigSpec._

  def is = s2"""
   Config parse should be able to parse
    minimal kafka config $minimal
  """

  private def minimal = {
    val path = Paths.get(getClass.getResource("/config.azure.minimal.hocon").toURI)
    ConfigParser.configFromFile[IO, Config[KafkaSourceConfig, KafkaSinkConfig]](path).value.map { result =>
      result must beRight(expectedMinimalConfig)
    }
  }
}

object KafkaConfigSpec {
  private val expectedMinimalConfig = Config[KafkaSourceConfig, KafkaSinkConfig](
    input = KafkaSourceConfig(
      topicName        = "sp-dev-enriched",
      bootstrapServers = "localhost:9092",
      consumerConf = Map(
        "group.id" -> "snowplow-databricks-loader",
        "allow.auto.create.topics" -> "false",
        "auto.offset.reset" -> "latest",
        "security.protocol" -> "SASL_SSL",
        "sasl.mechanism" -> "OAUTHBEARER",
        "sasl.jaas.config" -> "org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;",
        "group.instance.id" -> "test-hostname"
      ),
      debounceCommitOffsets = 10.seconds
    ),
    output = Config.Output(
      good = Config.Databricks(
        host        = "https://<identifier>.cloud.databricks.com",
        token       = None,
        oauth       = Some(Config.DatabricksOAuth(clientId = "test-client-id", clientSecret = "test-secret")),
        catalog     = "snowplow",
        schema      = "atomic",
        volume      = "snowplow",
        compression = CompressionCodecName.SNAPPY
      ),
      bad = Config.SinkWithMaxSize(
        sink = KafkaSinkConfig(
          topicName        = "sp-dev-bad",
          bootstrapServers = "localhost:9092",
          producerConf = Map(
            "client.id" -> "snowplow-databricks-loader",
            "security.protocol" -> "SASL_SSL",
            "sasl.mechanism" -> "OAUTHBEARER",
            "sasl.jaas.config" -> "org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;"
          )
        ),
        maxRecordSize = 1000000
      )
    ),
    batching = Config.Batching(
      maxBytes          = 16000000,
      maxDelay          = 1.second,
      uploadConcurrency = 3
    ),
    retries = Config.Retries(
      setupErrors     = Retrying.Config.ForSetup(delay = 30.seconds),
      transientErrors = Retrying.Config.ForTransient(delay = 1.second, attempts = 5)
    ),
    telemetry = Telemetry.Config(
      disable         = false,
      interval        = 15.minutes,
      collectorUri    = "collector-g.snowplowanalytics.com",
      collectorPort   = 443,
      secure          = true,
      userProvidedId  = None,
      autoGeneratedId = None,
      instanceId      = None,
      moduleName      = None,
      moduleVersion   = None
    ),
    monitoring = Config.Monitoring(
      metrics     = Config.Metrics(None),
      sentry      = None,
      healthProbe = Config.HealthProbe(port = Port.fromInt(8000).get, unhealthyLatency = 5.minutes),
      webhook     = Webhook.Config(endpoint = None, tags = Map.empty, heartbeat = 5.minutes)
    ),
    license                 = AcceptedLicense(),
    skipSchemas             = List.empty,
    exitOnMissingIgluSchema = true,
    http                    = Config.Http(HttpClient.Config(4)),
    dev                     = Config.DevFeatures(setEtlTstamp = false)
  )
}
