/*
 * Copyright (c) 2012-present Snowplow Analytics Ltd.
 * All rights reserved.
 *
 * This software is made available by Snowplow Analytics, Ltd.,
 * under the terms of the Snowplow Limited Use License Agreement, Version 1.1
 * located at https://docs.snowplow.io/limited-use-license-1.1
 * BY INSTALLING, DOWNLOADING, ACCESSING, USING OR DISTRIBUTING ANY PORTION
 * OF THE SOFTWARE, YOU AGREE TO THE TERMS OF SUCH LICENSE AGREEMENT.
 */

package com.snowplowanalytics.snowplow.databricks

import cats.Id
import cats.effect.testing.specs2.CatsEffect
import cats.effect.IO
import org.http4s.implicits._
import com.comcast.ip4s.Port
import org.apache.parquet.hadoop.metadata.CompressionCodecName
import org.specs2.Specification

import java.nio.file.Paths
import scala.concurrent.duration.DurationInt

import com.snowplowanalytics.iglu.core.SchemaCriterion

import com.snowplowanalytics.snowplow.runtime.{
  AcceptedLicense,
  ConfigParser,
  HttpClient,
  Metrics => CommonMetrics,
  Retrying,
  Telemetry,
  Webhook
}
import com.snowplowanalytics.snowplow.streams.pubsub.{
  GcpUserAgent => PubsubUserAgent,
  PubsubFactoryConfig,
  PubsubSinkConfig,
  PubsubSinkConfigM,
  PubsubSourceConfig
}

class PubsubConfigSpec extends Specification with CatsEffect {
  import PubsubConfigSpec._

  def is = s2"""
   Config parse should be able to parse
    minimal pubsub config $minimal
    extended pubsub config $extended
  """

  private def minimal = {
    val path = Paths.get(getClass.getResource("/config.pubsub.minimal.hocon").toURI)
    ConfigParser.configFromFile[IO, Config[PubsubFactoryConfig, PubsubSourceConfig, PubsubSinkConfig]](path).value.map { result =>
      result must beRight(expectedMinimalConfig)
    }
  }

  private def extended = {
    val path = Paths.get(getClass.getResource("/config.pubsub.reference.hocon").toURI)
    ConfigParser.configFromFile[IO, Config[PubsubFactoryConfig, PubsubSourceConfig, PubsubSinkConfig]](path).value.map { result =>
      result must beRight(expectedExtendedConfig)
    }
  }
}

object PubsubConfigSpec {
  private val expectedMinimalConfig = Config[PubsubFactoryConfig, PubsubSourceConfig, PubsubSinkConfig](
    input = PubsubSourceConfig(
      subscription            = PubsubSourceConfig.Subscription("my-project", "snowplow-enriched"),
      parallelPullFactor      = BigDecimal(0.5),
      durationPerAckExtension = 15.seconds,
      minRemainingAckDeadline = BigDecimal(0.1),
      maxMessagesPerPull      = 1000,
      debounceRequests        = 100.millis,
      streamingPull           = true,
      retries                 = PubsubSourceConfig.Retries(PubsubSourceConfig.TransientErrorRetrying(delay = 100.millis, attempts = 10))
    ),
    output = Config.Output(
      good = Config.Databricks(
        host        = "https://<identifier>.cloud.databricks.com",
        token       = None,
        oauth       = Some(Config.DatabricksOAuth(clientId = "test-client-id", clientSecret = "test-secret")),
        catalog     = "snowplow",
        schema      = "atomic",
        volume      = "snowplow",
        compression = CompressionCodecName.SNAPPY
      ),
      bad = Config.SinkWithMaxSize(
        sink = PubsubSinkConfigM[Id](
          topic                = PubsubSinkConfig.Topic("my-project", "snowplow-bad"),
          batchSize            = 1000,
          requestByteThreshold = 1000000,
          retries              = PubsubSinkConfig.Retries(PubsubSinkConfig.TransientErrorRetrying(delay = 100.millis, attempts = 10))
        ),
        maxRecordSize = 9000000
      )
    ),
    streams = PubsubFactoryConfig(PubsubUserAgent("Snowplow OSS", "databricks-loader"), None),
    batching = Config.Batching(
      maxBytes                = 16000000,
      maxDelay                = 1.second,
      uploadParallelismFactor = 3
    ),
    retries = Config.Retries(
      setupErrors     = Retrying.Config.ForSetup(delay = 30.seconds),
      transientErrors = Retrying.Config.ForTransient(delay = 1.second, attempts = 5)
    ),
    telemetry = Telemetry.Config(
      disable         = false,
      interval        = 15.minutes,
      collectorUri    = uri"https://collector-g.snowplowanalytics.com",
      userProvidedId  = None,
      autoGeneratedId = None,
      instanceId      = None,
      moduleName      = None,
      moduleVersion   = None
    ),
    cpuParallelismFactor = 0.75,
    monitoring = Config.Monitoring(
      metrics     = Config.Metrics(None),
      sentry      = None,
      healthProbe = Config.HealthProbe(port = Port.fromInt(8000).get, unhealthyLatency = 5.minutes),
      webhook     = Webhook.Config(endpoint = None, tags = Map.empty, heartbeat = 5.minutes)
    ),
    license                 = AcceptedLicense(),
    skipSchemas             = List.empty,
    exitOnMissingIgluSchema = true,
    http                    = Config.Http(HttpClient.Config(4)),
    dev                     = Config.DevFeatures(setEtlTstamp = false)
  )

  private val expectedExtendedConfig = Config[PubsubFactoryConfig, PubsubSourceConfig, PubsubSinkConfig](
    input = PubsubSourceConfig(
      subscription            = PubsubSourceConfig.Subscription("myproject", "snowplow-enriched"),
      parallelPullFactor      = BigDecimal(0.5),
      durationPerAckExtension = 15.seconds,
      minRemainingAckDeadline = BigDecimal(0.1),
      maxMessagesPerPull      = 1000,
      debounceRequests        = 100.millis,
      streamingPull           = true,
      retries                 = PubsubSourceConfig.Retries(PubsubSourceConfig.TransientErrorRetrying(delay = 100.millis, attempts = 10))
    ),
    output = Config.Output(
      good = Config.Databricks(
        host        = "https://<identifier>.cloud.databricks.com",
        token       = Some("test-token"),
        oauth       = Some(Config.DatabricksOAuth(clientId = "test-client-id", clientSecret = "test-secret")),
        catalog     = "snowplow",
        schema      = "atomic",
        volume      = "snowplow",
        compression = CompressionCodecName.SNAPPY
      ),
      bad = Config.SinkWithMaxSize(
        sink = PubsubSinkConfigM[Id](
          topic                = PubsubSinkConfig.Topic("myproject", "snowplow-bad"),
          batchSize            = 100,
          requestByteThreshold = 1000000,
          retries              = PubsubSinkConfig.Retries(PubsubSinkConfig.TransientErrorRetrying(delay = 100.millis, attempts = 10))
        ),
        maxRecordSize = 9000000
      )
    ),
    streams = PubsubFactoryConfig(PubsubUserAgent("Snowplow OSS", "databricks-loader"), None),
    batching = Config.Batching(
      maxBytes                = 16000000,
      maxDelay                = 1.second,
      uploadParallelismFactor = 3
    ),
    cpuParallelismFactor = 0.75,
    retries = Config.Retries(
      setupErrors     = Retrying.Config.ForSetup(delay = 30.seconds),
      transientErrors = Retrying.Config.ForTransient(delay = 1.second, attempts = 5)
    ),
    telemetry = Telemetry.Config(
      disable         = false,
      interval        = 15.minutes,
      collectorUri    = uri"https://collector-g.snowplowanalytics.com",
      userProvidedId  = Some("my_pipeline"),
      autoGeneratedId = Some("hfy67e5ydhtrd"),
      instanceId      = Some("665bhft5u6udjf"),
      moduleName      = Some("databricks-loader-vmss"),
      moduleVersion   = Some("1.0.0")
    ),
    monitoring = Config.Monitoring(
      metrics = Config.Metrics(
        statsd = Some(
          CommonMetrics.StatsdConfig(
            hostname = "127.0.0.1",
            port     = 8125,
            tags     = Map("myTag" -> "xyz"),
            period   = 1.minute,
            prefix   = "snowplow.databricks-loader"
          )
        )
      ),
      sentry = Some(Config.SentryM[Id](dsn = "https://public@sentry.example.com/1", tags = Map("myTag" -> "xyz"))),
      healthProbe = Config.HealthProbe(
        port             = Port.fromInt(8000).get,
        unhealthyLatency = 5.minutes
      ),
      webhook =
        Webhook.Config(endpoint = Some(uri"https://webhook.acme.com"), tags = Map("pipeline" -> "production"), heartbeat = 60.minutes)
    ),
    license = AcceptedLicense(),
    skipSchemas = List(
      SchemaCriterion.parse("iglu:com.acme/skipped1/jsonschema/1-0-0").get,
      SchemaCriterion.parse("iglu:com.acme/skipped2/jsonschema/1-0-*").get,
      SchemaCriterion.parse("iglu:com.acme/skipped3/jsonschema/1-*-*").get,
      SchemaCriterion.parse("iglu:com.acme/skipped4/jsonschema/*-*-*").get
    ),
    exitOnMissingIgluSchema = true,
    http                    = Config.Http(HttpClient.Config(4)),
    dev                     = Config.DevFeatures(setEtlTstamp = false)
  )
}
